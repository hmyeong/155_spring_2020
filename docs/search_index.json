[
["index.html", "STAT 155: Introduction to Statistical Modeling Welcome!", " STAT 155: Introduction to Statistical Modeling Welcome! Image source: xkcd This is the class manual for Introduction to Statistical Modeling (STAT 155) at Macalester College for the Spring 2020 semester. The content here was developed by Leslie Myint and draws upon material developed by the Macalester statistics faculty. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["schedule.html", "Schedule Week 1: 1/24 Week 2: 1/27 - 1/31 Week 3: 2/3 - 2/7 Week 4: 2/10 - 2/14", " Schedule Week 1: 1/24 Friday: Introductions; a taste of R and visualization Week 2: 1/27 - 1/31 Monday: Univariate visualization and summary measures Before class: Follow all instructions on the Software Setup page. Read sections 1.1, 2.1-2.4, and 2.6 in the STAT 155 Notes, and answer the Moodle questions under Reading Checks &gt; Data; Univariate Summaries and Visualization. Wednesday: Bivariate visualization and summary measures Before class: Read sections 2.5, 2.7, and 2.8 in the STAT 155 Notes, and answer the Moodle questions under Reading Checks &gt; Bivariate Summaries and Visualization. Friday: Continue with bivariate visualization. No new reading or Moodle questions. Just work on Homework 1. Homework 1 (on Moodle) is due Friday, January 31 at midnight. Week 3: 2/3 - 2/7 Monday: Simple linear regression models Before class: Read sections 3.1 to 3.6 in the STAT 155 Notes, and answer the Moodle questions under Reading Checks &gt; Simple Linear Regression (Part 1). Wednesday: Model evaluation and categorical predictors Before class: Read section 3.8 in the STAT 155 Notes only up through section 3.8.1 Indicator Variables, and answer the Moodle questions under Reading Checks &gt; Simple Linear Regression (Part 2). Friday: Quiz 1 in class Homework 2 (on Moodle) is due Thursday, February 13 at midnight. Week 4: 2/10 - 2/14 Reminder: Homework 2 (on Moodle) is due Thursday, February 13 at midnight. Monday: Multiple linear regression models Before class: Read sections 3.8.1 (Indicator Variables), 3.8.3 (Causation), and 3.8.6 (Dealing with Non-Linear Relationships) in the STAT 155 Notes, and answer the Moodle questions under Reading Checks &gt; Multiple Linear Regression. Wednesday: Multiple linear regression models Before class: No new reading. Friday: Interaction models Before class: Read section 3.8.2 (Interaction Variables) in the STAT 155 Notes, and answer the Moodle questions under Reading Checks &gt; Interaction Models. "],
["software-setup.html", "Software Setup Troubleshooting", " Software Setup Before class on Monday, January 27, you should follow these instructions to set up the software that we’ll be using throughout the semester. Highly recommended: Change the default file download location for your internet browser Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. Required: Please watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) Quit RStudio. You’re done setting up! Optional: If you want a quick tour of RStudio before class on Monday, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(&quot;rlang&quot;) in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],
["welcome-and-motivation.html", "Topic 1 Welcome and Motivation Introductions A taste of data visualization in R", " Topic 1 Welcome and Motivation Introductions Get to know the others at your table. Share your names, preferred pronouns, and anything else that is important to you. A taste of data visualization in R Throughout the semester we’ll be using the statistical software R to analyze data. We’ll take a first look at R code today in the following activity. Board games are a favorite hobby of your instructor, and to her delight, there is a dataset containing information about many different board games and their ratings from the website BoardGameGeek.com! You will work through exploring this dataset in pairs, and the instructor will be circling around to help. For some phases of this exploration, you will need to have this page open to access the data description. This code may seem foreign at first, but over the semester you will have a lot of practice, and you’ll be able to write code like this by yourself at the end! Today, focus on getting a feel for what the code looks like and its potential for helping us gain insight. Warm-up The data are stored in an object (a container) called games. Let’s get a quick feel for this data. How many variables are present? Which are quantitative? Which are categorical? How many cases, or units of observation, do we have? How are the variables named / labeled? eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZ2dwbG90MilcbmxpYnJhcnkoZHBseXIpXG5saWJyYXJ5KHJlYWRyKVxuZ2FtZXMgPC0gcmVhZHI6OnJlYWRfY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3Jmb3JkYXRhc2NpZW5jZS90aWR5dHVlc2RheS9tYXN0ZXIvZGF0YS8yMDE5LzIwMTktMDMtMTIvYm9hcmRfZ2FtZXMuY3N2XCIpIiwic2FtcGxlIjoiIyBMb29rIGF0IHRoZSBmaXJzdCA2IHJvd3NcbmhlYWQoZ2FtZXMpXG5cbiMgSG93IG1hbnkgcm93cyBhbmQgY29sdW1ucyBkb2VzIHRoZSBkYXRhIGhhdmU/XG4jIFdoYXQgYXJlIGl0cyBkaW1lbnNpb25zP1xuZGltKGdhbWVzKSJ9 Phase 1 Use the code below to construct a visualization (a histogram) of average ratings for the games. What do you notice? What information can you gain from this plot? What do you think binwidth = 2 does? Try setting a different value for binwidth to make a plot that looks better to you. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZ2dwbG90MilcbmxpYnJhcnkoZHBseXIpXG5saWJyYXJ5KHJlYWRyKVxuZ2FtZXMgPC0gcmVhZHI6OnJlYWRfY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3Jmb3JkYXRhc2NpZW5jZS90aWR5dHVlc2RheS9tYXN0ZXIvZGF0YS8yMDE5LzIwMTktMDMtMTIvYm9hcmRfZ2FtZXMuY3N2XCIpIiwic2FtcGxlIjoiIyBDcmVhdGUgYSBwbG90IG9mIGF2ZXJhZ2UgcmF0aW5ncyAoaGlzdG9ncmFtKVxuZ2dwbG90KGdhbWVzLCBhZXMoeCA9IGF2ZXJhZ2VfcmF0aW5nKSkgK1xuICAgIGdlb21faGlzdG9ncmFtKGJpbndpZHRoID0gMikifQ== Phase 2 Another way to visualize average ratings is shown below. Use the code below to construct a density plot of average ratings. Do you like this plot or the previous one better? Why? eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZ2dwbG90MilcbmxpYnJhcnkoZHBseXIpXG5saWJyYXJ5KHJlYWRyKVxuZ2FtZXMgPC0gcmVhZHI6OnJlYWRfY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3Jmb3JkYXRhc2NpZW5jZS90aWR5dHVlc2RheS9tYXN0ZXIvZGF0YS8yMDE5LzIwMTktMDMtMTIvYm9hcmRfZ2FtZXMuY3N2XCIpIiwic2FtcGxlIjoiIyBDcmVhdGUgYSBwbG90IG9mIGF2ZXJhZ2UgcmF0aW5ncyAoZGVuc2l0eSBwbG90KVxuZ2dwbG90KGdhbWVzLCBhZXMoeCA9IGF2ZXJhZ2VfcmF0aW5nKSkgK1xuICAgIGdlb21fZGVuc2l0eSgpIn0= Phase 3 Using code similar to either of the two above examples in Phases 1 and 2, what other variables might you visualize in a similar way? Try the code here. Make note of any interesting findings. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZ2dwbG90MilcbmxpYnJhcnkoZHBseXIpXG5saWJyYXJ5KHJlYWRyKVxuZ2FtZXMgPC0gcmVhZHI6OnJlYWRfY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3Jmb3JkYXRhc2NpZW5jZS90aWR5dHVlc2RheS9tYXN0ZXIvZGF0YS8yMDE5LzIwMTktMDMtMTIvYm9hcmRfZ2FtZXMuY3N2XCIpIiwic2FtcGxlIjoiIyBWaXN1YWxpemUgc29tZSBvdGhlciB2YXJpYWJsZXMifQ== Phase 4 Using the code below, construct a visualization of average ratings broken down by minimum number of players. What do you notice? What is surprising? What is not surprising? eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZ2dwbG90MilcbmxpYnJhcnkoZHBseXIpXG5saWJyYXJ5KHJlYWRyKVxuZ2FtZXMgPC0gcmVhZHI6OnJlYWRfY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3Jmb3JkYXRhc2NpZW5jZS90aWR5dHVlc2RheS9tYXN0ZXIvZGF0YS8yMDE5LzIwMTktMDMtMTIvYm9hcmRfZ2FtZXMuY3N2XCIpIiwic2FtcGxlIjoiIyBWaXN1YWxpemUgYXZlcmFnZSByYXRpbmdzIGZvciBkaWZmZXJlbnQgbnVtYmVycyBvZiBtaW4uIHBsYXllcnNcbmdncGxvdChnYW1lcywgYWVzKHggPSBhdmVyYWdlX3JhdGluZykpICtcbiAgICBnZW9tX2hpc3RvZ3JhbSgpICtcbiAgICBmYWNldF93cmFwKH4gbWluX3BsYXllcnMpIn0= Phase 5 Using the code below, construct two alternate visualizations of average ratings broken down by minimum number of players. These plots use color instead of panels and also only keep games for which the minimum number of players ranges from 1 to 4. Which of the three visualizations do you like best? Why? eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZ2dwbG90MilcbmxpYnJhcnkoZHBseXIpXG5saWJyYXJ5KHJlYWRyKVxuZ2FtZXMgPC0gcmVhZHI6OnJlYWRfY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3Jmb3JkYXRhc2NpZW5jZS90aWR5dHVlc2RheS9tYXN0ZXIvZGF0YS8yMDE5LzIwMTktMDMtMTIvYm9hcmRfZ2FtZXMuY3N2XCIpIiwic2FtcGxlIjoiIyBTdWJzZXQgdGhlIGRhdGEgdG8gb25seSBrZWVwIGdhbWVzIHdoZXJlIG1pbl9wbGF5ZXJzIGlzIGZyb20gMSB0byA0XG5nYW1lc19zdWIgPC0gZHBseXI6OmZpbHRlcihnYW1lcywgbWluX3BsYXllcnMgPj0gMSwgbWluX3BsYXllcnMgPD0gNClcblxuIyBDb2xvcmVkIGhpc3RvZ3JhbXNcbmdncGxvdChnYW1lc19zdWIsIGFlcyh4ID0gYXZlcmFnZV9yYXRpbmcsIGZpbGwgPSBmYWN0b3IobWluX3BsYXllcnMpKSkgK1xuICAgIGdlb21faGlzdG9ncmFtKClcblxuIyBDb2xvcmVkIGRlbnNpdHkgcGxvdHNcbmdncGxvdChnYW1lc19zdWIsIGFlcyh4ID0gYXZlcmFnZV9yYXRpbmcsIGNvbG9yID0gZmFjdG9yKG1pbl9wbGF5ZXJzKSkpICtcbiAgICBnZW9tX2RlbnNpdHkoKSJ9 Phase 6 Using the code below, construct a visualization of the relationship between minimum recommended play time and average rating (a scatterplot). Is the plot useful? Why or why not? Using the dplyr::filter part of the Phase 5 code, subset the data to only keep games where the minimum recommended playtime is less than 1000 minutes. Then remake the scatterplot. Make note of any interesting findings. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZ2dwbG90MilcbmxpYnJhcnkoZHBseXIpXG5saWJyYXJ5KHJlYWRyKVxuZ2FtZXMgPC0gcmVhZHI6OnJlYWRfY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3Jmb3JkYXRhc2NpZW5jZS90aWR5dHVlc2RheS9tYXN0ZXIvZGF0YS8yMDE5LzIwMTktMDMtMTIvYm9hcmRfZ2FtZXMuY3N2XCIpIiwic2FtcGxlIjoiIyBSZWxhdGlvbnNoaXAgYmV0d2VlbiBtaW4uIHJlY29tbWVuZGVkIHBsYXkgdGltZSBhbmQgYXZlcmFnZSByYXRpbmdcbmdncGxvdChnYW1lcywgYWVzKHggPSBtaW5fcGxheXRpbWUsIHkgPSBhdmVyYWdlX3JhdGluZykpICtcbiAgICBnZW9tX3BvaW50KCkgK1xuICAgIGdlb21fc21vb3RoKClcblxuIyBTdWJzZXQgdGhlIGRhdGEgdG8gb25seSBrZWVwIGdhbWVzIHdoZXJlIHRoZVxuIyBtaW5pbXVtIHJlY29tbWVuZGVkIHBsYXl0aW1lIGlzIGxlc3MgdGhhbiAxMDAwIG1pbnV0ZXNcblxuXG4jIFJlbGF0aW9uc2hpcCBiZXR3ZWVuIG1pbi4gcmVjb21tZW5kZWQgcGxheSB0aW1lIGFuZCBhdmVyYWdlIHJhdGluZyJ9 Phase 7 What other questions would you like to explore? Try them out below. If you would like to try something that is not part of the above examples, feel free to brainstorm with the instructor. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImxpYnJhcnkoZ2dwbG90MilcbmxpYnJhcnkoZHBseXIpXG5saWJyYXJ5KHJlYWRyKVxuZ2FtZXMgPC0gcmVhZHI6OnJlYWRfY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3Jmb3JkYXRhc2NpZW5jZS90aWR5dHVlc2RheS9tYXN0ZXIvZGF0YS8yMDE5LzIwMTktMDMtMTIvYm9hcmRfZ2FtZXMuY3N2XCIpIiwic2FtcGxlIjoiIyBZb3VyIG93biBleHBsb3JhdGlvbnMhIn0= "],
["univariate-visualization-and-data-summaries.html", "Topic 2 Univariate Visualization and Data Summaries Learning Goals Discussion Exercises", " Topic 2 Univariate Visualization and Data Summaries Learning Goals Understand how bar charts, histograms, and density plots are constructed (not in full detail for a density plot) Identify the best type of plot to display a given variable and be able to construct those plots in R Clearly describe plots of quantitative variables using the concepts of shape, center, spread, and outliers Relate summary statistics of data to the concepts of shape, center, spread, and outliers Discussion When you interpret a plot of a quantitative variable, I expect you to discuss the following four aspects as part of a complete answer. Shape: How are values distributed along the observed range? What does the distribution of the variable look like? games &lt;- read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv&quot;) ggplot(games, aes(x = min_playtime)) + geom_density() Center: What is a typical value of the variable? Quantified with summary statistics like the mean and median. summary(games$min_playtime) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 25.00 45.00 80.88 90.00 60000.00 Spread: How spread out are the values? Are most values very close together or far apart? Quantified with summary statistics like the variance and standard deviation. Interpretation of the variance: it is the average (roughly) squared distance of each value to the mean. Units are the squared version of the original variable. Interpretation of the standard deviation: square root of the variance. Measures spread on the same scale as the original variable (same units as the original variable). var(games$min_playtime) ## [1] 406883.1 sd(games$min_playtime) ## [1] 637.8739 Outliers: Are there any values that are particularly high or low relative to the rest? A good paragraph putting all of these aspects together: The distribution of minimum playtimes is right-skewed with values ranging from 0 to 60,000 minutes. The center of the distribution is around 45 minutes (median). The mean is higher at about 80.88 minutes because of the high outliers (such as the game with a 60,000 minute minimum play time). Because of the outliers, it is difficult to see the spread of the bulk of the data clearly, and it is these outliers that contribute to the high standard deviation of 637.9 minutes. Qualities of a good paragraph: Describes the shape and range of the data Includes units Reports on the mean and median and explains any discrepancies between them Describes spread in terms of standard deviation Comments on the noteworthy visual features of the plot, including outliers Exercises A template RMarkdown document that you can start from is available here. As you proceed, put any new code that you encounter on your cheat sheet. Part 1 We have data on course evaluations for all courses offered in the Spring 2019 semester at Johns Hopkins University (source). We have the following pieces of information on each course: avg_rating: The average of student responses to the question asking them to rate the overall quality of the course using the Likert scale: 1 = Poor 2 = Weak 3 = Fair 4 = Good 5 = Excellent division: Either AS for Arts &amp; Sciences or EN for Engineering dept_name: Name of the department # Load required packages library(readr) library(ggplot2) # Read in the course evaluation data evals &lt;- read_csv(&quot;https://www.dropbox.com/s/3gayi1iq2p76kn0/jhu_evals.csv?dl=1&quot;) Exercise 1 The first step in any data analysis is to get acquainted with the data. Look at the first few rows, and obtain the dimensions of the dataset. # Look at the first 6 rows head(evals) # Get the dimensions of the dataset dim(evals) How many cases are there? (What are the cases?) What type of variable are the key ones described above? Exercise 2 Are the numbers of courses taught in the Arts &amp; Sciences and the Engineering divisions roughly the same? Before making the plot, make note of what your expectation is. Then compare to what you observe when you make the plot. (Clearly defining your expectations first is good scientific practice that is often not practiced enough to become habit.) ggplot(evals, aes(x = division)) + geom_bar() Exercise 3 Now we’ll look at the distribution of the avg_rating variable and associated summary statistics. Just like before, make note of what you expect the plots to look like before you make them. # Plots ggplot(evals, aes(x = avg_rating)) + geom_histogram() ggplot(evals, aes(x = avg_rating)) + geom_density() # Summary statistics # Adapt the code from the Discussion section to compute these ??? Write a good paragraph interpreting the histogram or density plot. (Discuss the 4 essential aspects.) What information is given by the tallest bar of the histogram? In what situations might you prefer a histogram to a density plot and vice-versa? Looking at the summary statistics, compare the mean to the median, and relate this to the shape of the distribution of average ratings. What would the distribution probably look like if the ordering of the mean and median were reversed? Part 2 When we are sitting behind a computer screen analyzing data, it can be easy to get caught up in the code, methods, or findings, and lose track of the human impact of our analysis. The analysis we did in Part 1 actually foreshadows a rather serious ethical concern for faculty. Exercise 4 Suppose that you were asked to rate the overall quality of one of your Fall courses on a 5-point scale. What kinds of things would you think about in giving your rating? Do you think you would give the same rating on a different day? In your opinion, do you think other students are using the same criteria as you? Exercise 5 In this last exercise, we’ll explore the misleading nature of the mean of Likert-scale responses. The datasets being read in below come from 3 different courses with 20 students each. For each student we have their rating of the overall course quality on the scale: 1 = Poor 2 = Weak 3 = Fair 4 = Good 5 = Excellent # Read in the course evaluation data course1 &lt;- read_csv(&quot;https://www.dropbox.com/s/qbw4ahys9mkxsqj/course1.csv?dl=1&quot;) course2 &lt;- read_csv(&quot;https://www.dropbox.com/s/tg8g0hchh435lx7/course2.csv?dl=1&quot;) course3 &lt;- read_csv(&quot;https://www.dropbox.com/s/b3cg2tdbpyjzwr6/course3.csv?dl=1&quot;) For each course, make an appropriate plot showing the distribution of student ratings. Put yourself in the professor’s shoes and briefly summarize the information that the plot gives. # Plot the distribution of ratings for each course ??? Now compute the mean rating for each course. You can adapt the code from the Discussion section to replace summary with mean to display only the mean. # Compute the mean rating for each course ??? Consider a school policy that ranks professors based on this mean rating and uses the rankings for evaluative and promotion purposes. Based on your investigations so far, would such a policy be fair? Is there a potential for it creating injustice? Extra! If you have time and want to explore more, try these exercises. Make a plot showing the number of courses by department. Is it effective? Add the following to the end of the code for your last plot. What does this code do? Add this to your cheat sheet. + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Continue adding to the same plot the following code. What does it do? Modify it so that it is better suited to your plot. Add this to your cheat sheet. + labs(x = &quot;xxx&quot;, y = &quot;yyy&quot;, title = &quot;title&quot;) "],
["bivariate-visualization-part-1.html", "Topic 3 Bivariate Visualization - Part 1 Learning Goals Exercises", " Topic 3 Bivariate Visualization - Part 1 Learning Goals Construct bivariate data visualizations of (1) two categorical variables and (2) one quantitative and one categorical variable Using good visualization principles, compare the strengths and weaknesses of these different visualizations Exercises A template RMarkdown document that you can start from is available here. As you proceed, put any new code that you encounter on your cheat sheet. Context and Setup Today we begin our data exploration for our first case study! We’ll be looking at a dataset of weightlifting competition results to understand how various factors relate to an athlete’s strength. The data originally come from Kaggle and OpenPowerlifting. # Load required packages library(readr) library(ggplot2) library(dplyr) # Read in the weightlifting data lifts &lt;- read_csv(&quot;https://www.dropbox.com/s/n6jko5m7ygeasoj/openpowerlifting_subs.csv?dl=1&quot;) Exercise 1 As always, we’ll start by getting acquainted with the data. Look at the first few rows, and obtain the dimensions of the dataset. # Look at the first 6 rows # Get the dimensions of the dataset How many cases are there? (What are the cases?) What kinds of variables do we have information on? (Are there broader categories of variables?) Look up any variables that are unfamiliar (for example, Wilks Coefficient). Exercise 2 Research question: Do males and females tend to use different types of equipment? The three barplots below give different ways of showing the Sex and Equipment variables. Which one is best suited to answering this research question and why? ggplot(lifts, aes(x = Sex, fill = Equipment)) + geom_bar() ggplot(lifts, aes(x = Sex, fill = Equipment)) + geom_bar(position = &quot;dodge&quot;) ggplot(lifts, aes(x = Sex, fill = Equipment)) + geom_bar(position = &quot;fill&quot;) Exercise 3 Your client for this case study (Leslie) is interested in using two of the variables to compute a third one. In particular, she wants to create a variable that is the ratio of total weight lifted to the athlete’s bodyweight. The code below creates a new variable called SWR (strength-to-weight ratio). Read through the code below and get a sense for what it is doing. If you have questions, ask the instructor. # The %&gt;% symbol is called a pipe. You can read it as: # take the output that comes before the pipe and feed it in as input to the function that comes after lifts &lt;- lifts %&gt;% mutate(SWR = TotalKg/BodyweightKg) Research question: How does strength-to-weight ratio differ between males and females? The code below makes different plots that can be used to explore this question. Before running the code, make note of what you expect. How might each contribute to answering the research question? Which one(s) would you choose to present to your client and why? ggplot(lifts, aes(x = SWR, fill = Sex)) + geom_histogram() ggplot(lifts, aes(x = SWR, fill = Sex)) + geom_histogram() + facet_grid(~ Sex) ggplot(lifts, aes(x = SWR, color = Sex)) + geom_density() ggplot(lifts, aes(x = Sex, y = SWR)) + geom_boxplot() Exercise 4 Did you notice anything unusual about the shape of the distribution of SWR in males and females? What group of athletes do you think is causing this peculiar shape? (Hint: you may have noticed this when you looked at the first few rows of the data.) Let’s filter out these individuals and remake your preferred plots from Exercise 3. Do you think that the rest of the analysis should proceed with these athletes filtered out? # Fill in the ??? below with a description of the athletes that you want to KEEP # See Day 1 - Phase 5 for an example lifts_subs &lt;- lifts %&gt;% filter(???) # Now remake your preferred plots from Exercise 3 Extra! If you have time and want to continue your explorations, try the following: Make plots showing the relationship between SWR and Equipment. Make plots showing the relationship between Wilks and Sex. Make plots showing the relationship between Wilks and Equipment. "],
["bivariate-visualization-part-2.html", "Topic 4 Bivariate Visualization - Part 2 Learning Goals Exercises", " Topic 4 Bivariate Visualization - Part 2 Learning Goals Construct bivariate data visualizations of two quantiative variables Form ideas about the uses and building of statistical models Explain the uses, misuses, and limitations of the correlation coefficient Exercises A template RMarkdown document that you can start from is available here. We’ll continue using the dataset for our first case study: # Load required packages library(readr) library(ggplot2) library(dplyr) # Read in the weightlifting data lifts &lt;- read_csv(&quot;https://www.dropbox.com/s/n6jko5m7ygeasoj/openpowerlifting_subs.csv?dl=1&quot;) # Try to create the SWR variable without looking at your cheat sheet ??? Exercise 1 Using the plotting examples that we have gone over so far, generalize the code structure to create a scatterplot of SWR (y-axis) vs. BodyweightKg (x-axis). For no reason at all, I’ll say the word “point”. Before running the code, write down what you expect the plot to look like. In a few sentences describe the direction, form, strength, and any unusual features of the plot. Now add the following after the scatterplot: + geom_smooth(). What does this code do? Create another copy of your scatterplot code using geom_smooth(method = &quot;lm&quot;) instead of geom_smooth(). What does the argument method = &quot;lm&quot; do? Compute the correlation coefficient between these two variables using the following. Is the correlation coefficient appropriate for summarizing this relationship? cor(lifts$SWR, lifts$BodyweightKg) Pause after this exercise, and we’ll discuss as a class. Exercise 2 Using the plot from exercise 1, try to estimate by hand the equation of the line shown on the plot. Exercise 3 Now use the code below to more precisely get the equation of the line. More specifically, this code fits a simple linear regression model to the data. There is a lot of output that gets printed, but focus only on the “Coefficients:” section and the “Estimate” column in that section. How does your estimated line compare? mod1 &lt;- lm(SWR ~ BodyweightKg, data = lifts) summary(mod1) Exercise 4 Do you think the simple linear regression model is any good? What features of the plot contribute to your decision? Using tools that we’ve learned so far, how might you try to quantify the quality of this model? How could we make the model better? What’s missing from it? Exercise 5 Your client for this case study is interested in learning how height is related to athlete strength (as measured by SWR or Wilks. However, height is not recorded in the data. Plan: how might we address this issue? Exercise 6 This last exercise takes a step away from our lifts dataset. Load the anscombe dataset using: data(anscombe) There are 4 sets of data within this dataset. x1 and y1, x2 and y2, x3 and y3, and x4 and y4. Compute the correlation coefficient between each of these pairs, and also make a scatterplot for each of these pairs. What is the message of this last exercise? "],
["modeling-concepts-part-1.html", "Topic 5 Modeling Concepts (Part 1) Learning Goals New Groups! Discussion Exercises", " Topic 5 Modeling Concepts (Part 1) Learning Goals Construct simple linear regression models with a quantitative predictor in R Use these models to describe relationships and make predictions Develop two ideas of model quality: \\(R^2\\) and residual standard error Develop ideas of variation in estimates from sample to sample New Groups! In your new groups… Introduce yourselves Give your preferred pronouns If you had one extra hour of free time a day, how would you use it? Discussion Models A model is a simplified description of the world. Statistical models are mathematical descriptions that are informed by (estimated from) data. A simple linear regression model is a line model that can be mathematically represented as below: \\[ E[Y] = \\beta_0 + \\beta_1\\,X \\] \\(Y\\) is a quantitative response or outcome variable. e.g., House price \\(X\\) is a quantitative predictor or explanatory variable. e.g., Square footage of the house The \\(E[\\,\\,]\\) means “expected value of”: this is why lines model trends in averages and not individual-level trends \\(\\beta_0\\) and \\(\\beta_1\\) are the coefficients of the model. These coefficients are obtained from our data using the method of least squares. Interpreting the model coefficients When \\(X = 0\\), \\(E[Y] = \\beta_0\\). This means that \\(\\beta_0\\) is the expected value of the outcome when \\(X = 0\\). Note that this sometimes does not make sense! Centering the predictor can fix this. Don’t forget to report units in your interpretation! Case 1: \\(X = a \\qquad\\qquad\\qquad E[Y_1] = \\beta_0 + \\beta_1\\,a\\) Case 2: \\(X = a+1 \\qquad\\qquad E[Y_2] = \\beta_0 + \\beta_1\\,(a+1)\\) \\(E[Y_2] - E[Y_1] = \\beta_1\\) This means that \\(\\beta_1\\) is the expected change in the outcome per unit change in \\(X\\). Reporting units is important. The math shown here will be important when we extend these models to have multiple predictors. Fitting linear regression models in R # Fit the model and store the resulting information in &quot;mod&quot; mod &lt;- lm(y ~ x, data = your_dataset) # Display summary output of the model summary(mod) # Extract the residuals of the model residuals(mod) Exercises A template RMarkdown document that you can start from is available here. We will be looking at housing data from upstate New York. This dataset contains information on house price and structural characteristics for a large set of homes in Saratoga, NY. library(readr) library(ggplot2) library(dplyr) homes &lt;- read_tsv(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt&quot;) Exercise 1 Get to know the data by displaying the first few rows and getting the dimensions. What are the cases? How many cases do we have? What kinds of variables do we have information on? Exercise 2 Always start an analysis with visualization! Make a plot that shows the relationship between Price (response) and Living.Area (predictor). Do you think a simple linear regression model is appropriate? Compute the correlation between Price and Living.Area. Carefully explain what this tells you about the slope of the least squares line. Exercise 3 Fit a linear regression model of Price as a function of Living.Area using the code below. mod1 &lt;- lm(Price ~ Living.Area, data = homes) summary(mod1) The two numbers in the “Estimate” column of the output give the intercept and the slope for Living.Area. Interpret both of these numbers. Is the intercept meaningful? We can center Living.Area at a more reasonable value. Let’s use 1000 square feet. Complete the code below to create a centered version of Living.Area. homes &lt;- homes %&gt;% ???(Living.Area.Shifted = Living.Area-1000) You can actually determine the coefficients of the Price ~ Living.Area.Shifted model by hand. Based on the summary output of mod1, work out what these new coefficients should be. Now check your answer to part (d) by fitting the model. mod2 &lt;- lm(???) summary(mod2) What is the residual for a $150,000 house that is 1000 square feet? Exercise 4 In this exercise, we’ll develop some intuition for ideas about how to quantify the quality of a model. Based on your plot from Exercise 2, do you think that our Price ~ Living.Area model is a good one? Explain by describing whether you think the variance of the residuals should be low or high for a good model. In the model summary output, there is a metric near the bottom called “Residual standard error”. The first number (ignore the “on XXXX degrees of freedom part”) is very close to the standard deviation of the residuals of the model. Verify this with the code below. Given that residual standard error is essentially a standard deviation, how do you think we can interpret this number? mod1 %&gt;% residuals() %&gt;% sd() Do you think that the variance of the residuals could ever be larger than the variance of the house prices? Why or why not? There is a metric for linear regression models called \\(R^2\\) which is displayed in the model summary output near the bottom just after “Multiple R-squared”. This number is computed as: 1 - Var(residuals)/Var(response). Do you think this number should be high or low for a good model? # If you are curious about checking the calculation 1-(var(residuals(mod1))/var(homes$Price)) Exercise 5 We only used a sample of Saratoga homes to estimate $113.123 as the slope for Living.Area. Based on your scatterplot from Exercise 2, how much do you think that slope estimate would change if we had a different sample of houses? A lot? A little? What are you thinking about as you formulate your opinion? "],
["modeling-concepts-part-2.html", "Topic 6 Modeling Concepts (Part 2) Learning Goals Warm-up Discussion Exercises", " Topic 6 Modeling Concepts (Part 2) Learning Goals Practice simple linear regression modeling concepts: model formula, coefficient interpretations, predicted values, residuals Develop two ideas of model quality: \\(R^2\\) and residual standard error Understand how categorical predictors are incorporated in linear regression models Warm-up mod1 &lt;- lm(Price ~ Age, data = homes) summary(mod1) ## ## Call: ## lm(formula = Price ~ Age, data = homes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -222183 -66299 -22232 43147 564995 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 229728.46 3218.18 71.385 &lt; 2e-16 *** ## Age -636.26 79.66 -7.987 2.5e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 96700 on 1726 degrees of freedom ## Multiple R-squared: 0.03564, Adjusted R-squared: 0.03508 ## F-statistic: 63.79 on 1 and 1726 DF, p-value: 2.502e-15 Write the regression model formula using numbers from this output. Interpret all coefficients in this model. For a 50 year old house whose price is $100,000, what is the residual? Discussion What can we quantify about residuals to measure model quality? Not the sum or the mean of residuals (will always be zero) Residual standard error: essentially equal to the standard deviation of the residuals Scale of residual standard error changes with the scale of the data (e.g., house prices versus strength-to-weight ratio) Can we put the variance of the residuals on a nicer scale? Say from 0 to 1? Yes, we can. Some facts: \\[ \\hbox{Var}(\\hbox{response}) = \\hbox{Var}(\\hbox{residuals}) + \\hbox{Var}(\\hbox{predicted values}) \\] \\[ \\hbox{Total variation} = \\hbox{Unexplained variation} + \\hbox{Explained variation} \\] \\(R^2\\): What fraction of total variation in the response is explained by the model? Hopefully a lot. Which would mean that there is relatively little unexplained variation. Ranges from 0 to 1 \\[ \\begin{align*} R^2 &amp;= \\frac{\\hbox{Var}(\\hbox{predicted values})}{\\hbox{Var}(\\hbox{response})} \\\\ &amp;= 1 - \\frac{\\hbox{Var}(\\hbox{residuals})}{\\hbox{Var}(\\hbox{response})} \\end{align*} \\] mod1 &lt;- lm(Price ~ Living.Area, data = homes) summary(mod1) ## ## Call: ## lm(formula = Price ~ Living.Area, data = homes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -277022 -39371 -7726 28350 553325 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13439.394 4992.353 2.692 0.00717 ** ## Living.Area 113.123 2.682 42.173 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 69100 on 1726 degrees of freedom ## Multiple R-squared: 0.5075, Adjusted R-squared: 0.5072 ## F-statistic: 1779 on 1 and 1726 DF, p-value: &lt; 2.2e-16 Residual standard error: $69100 This describes the amount of spread in the residuals. What qualifies as “high”? Imagine that your residual changed by that much. Is that a lot? \\(R^2\\) (Multiple R-squared): 0.5075 50.75% of the variation in house prices is explained by a simple linear regression model with square footage as a predictor What qualifies as “high”? Context helps determine if the response variable simply varies a lot. (e.g., stocks) How do we incorporate categorical predictors? In our housing dataset, there is a Heat.Type that indicates whether the heating type of the house is of type 2, 3, or 4. Including a categorical predictor variable creates \\(L-1\\) indicator variables where \\(L\\) is the number of levels of the categorical variable. Type 2 is chosen as the reference category by default in R because it is first in alphanumeric order. Heat.Type3 and Heat.Type4 get created as indicator variables by taking the original variable name (Heat.Type) and pasting the name of the category (3 or 4 afterward) Heat.Type3 equals 1 is this case is of heating type 3. Equals 0 otherwise. Heat.Type4 equals 1 is this case is of heating type 4. Equals 0 otherwise. Case Heat.Type Heat.Type3 Heat.Type4 ---- --------- ---------- ---------- 1 3 1 0 2 4 0 1 3 4 0 1 4 2 0 0 mod2 &lt;- lm(Price ~ Heat.Type, data = homes) summary(mod2) ## ## Call: ## lm(formula = Price ~ Heat.Type, data = homes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -221355 -63355 -17644 43895 548645 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 226355 2853 79.348 &lt; 2e-16 *** ## Heat.Type3 -17223 6192 -2.781 0.00547 ** ## Heat.Type4 -64467 6168 -10.451 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 95510 on 1725 degrees of freedom ## Multiple R-squared: 0.05972, Adjusted R-squared: 0.05863 ## F-statistic: 54.78 on 2 and 1725 DF, p-value: &lt; 2.2e-16 From this output, I can see that the regression model formula is: \\[ \\begin{align*} E[\\hbox{Price}] &amp;= \\beta_0 + \\beta_1\\,\\hbox{Heat.Type3} + \\beta_2\\,\\hbox{Heat.Type4} \\\\ &amp;= 226355 - 17223\\,\\hbox{Heat.Type3} - 64467\\,\\hbox{Heat.Type4} \\end{align*} \\] When a house is of heating type 2, what are the values of the indicator variables? Thus what is the expected (average) price for a house of heating type 2? Same questions for types 3 and 4 This leads us to the interpretation of the coefficients in this model. Exercises We won’t be working in R today. Instead, look at the output from R code below, and answer the following questions. Exercise 1 Let’s look at a model that describes Price in terms of Fuel.Type, which can be of types 2, 3, or 4. mod3 &lt;- lm(Price ~ Fuel.Type, data = homes) summary(mod3) ## ## Call: ## lm(formula = Price ~ Fuel.Type, data = homes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -223535 -60535 -19652 42811 546465 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 228535 2748 83.160 &lt; 2e-16 *** ## Fuel.Type3 -63598 6021 -10.563 &lt; 2e-16 *** ## Fuel.Type4 -39801 7029 -5.663 1.74e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 95080 on 1725 degrees of freedom ## Multiple R-squared: 0.06823, Adjusted R-squared: 0.06715 ## F-statistic: 63.16 on 2 and 1725 DF, p-value: &lt; 2.2e-16 Interpret all coefficients in this model. Interpret the \\(R^2\\) and residual standard error to evaluate the quality of the model. What is the residual for a $250,000 house that is of fuel type 2? What about a $250,000 house that is of fuel type 3? Exercise 2 Let’s look at a model that describes Price in terms of Sewer.Type, which can be of types 1, 2, or 3. mod4 &lt;- lm(Price ~ Sewer.Type, data = homes) summary(mod4) ## ## Call: ## lm(formula = Price ~ Sewer.Type, data = homes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -211426 -66426 -21426 45574 574716 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 250952 28340 8.855 &lt;2e-16 *** ## Sewer.Type2 -50668 28676 -1.767 0.0774 . ## Sewer.Type3 -34527 28479 -1.212 0.2255 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 98170 on 1725 degrees of freedom ## Multiple R-squared: 0.006633, Adjusted R-squared: 0.005481 ## F-statistic: 5.759 on 2 and 1725 DF, p-value: 0.003215 Interpret all coefficients in this model. Interpret the \\(R^2\\) and residual standard error to evaluate the quality of the model. What is the residual for a $200,000 house that is of sewer type 3? What about a $200,000 house that is of sewer type 1? "],
["multiple-linear-regression.html", "Topic 7 Multiple Linear Regression Learning Goals Discussion and Exercises", " Topic 7 Multiple Linear Regression Learning Goals Understand the predictive and causal viewpoints for including multiple predictor variables in regression models Interpret the coefficients in multiple linear regression models Graphically/geometrically describe the “pictures” implied by different multivariate models Understand how to extend linear regression models with polynomial terms to model nonlinear relationships Discussion and Exercises A template RMarkdown document that you can start from is available here. New data context: diamond prices library(readr) library(ggplot2) library(dplyr) diamonds &lt;- read_csv(&quot;https://www.dropbox.com/s/9c8jqda4pwaq8i1/diamonds.csv?dl=1&quot;) Research question: How are different factors related to the price of a diamond? Let’s familiarize ourselves with the data: # Display the dimensions and first few rows Key variables: carat: the weight of the diamond in carats (1 carat = 200 milligrams) price: price in US dollars cut: quality of the cut of the diamond (Fair, Good, Very Good, Premium, Ideal) color: Level 1 (best) to Level 7 (worst) clarity: Level 1 (worst) to Level 8 (best) x, y, and z: length, width, and depth respectively (in mm) depth: total depth percentage = z / mean(x, y) table: width of top of diamond relative to widest point Exercise 1: What are your expectations for how price and clarity will be related? Construct a visualization to see. (Clarity level 1 is deemed worst and level 8 is deemed best.) # Plot of price vs. clarity Exercise 2: Let’s first fit a linear regression model with just clarity as a predictor of price. mod1 &lt;- lm(price ~ clarity, data = diamonds) summary(mod1) Why is there no coefficient for clarityLevel_1? Interpret the clarityLevel_2 and clarityLevel_5 coefficients. Based on this output, can you tell which clarity level has the highest mean price in this dataset? Explain. Relate the magnitudes of the coefficients to the trends that you see in your plot. Exercise 3: What if the most clear diamonds happened to be quite small (low carat)? Would this explain why your plot looks the way it does? Wouldn’t it be nice if we could hold carat fixed and then compare diamonds of different clarity? We can with regression models. If we want to examine average changes in the response while letting one variable change and holding the others fixed, we can add those other variables to the model: \\[ E[\\hbox{price}] = \\beta_0 + \\beta_1\\hbox{clarityLevel_2} + \\beta_2\\hbox{clarityLevel_3} + \\cdots + \\beta_7\\hbox{clarityLevel_8} + \\beta_8\\hbox{carat} \\] Let’s simplify this model to gain better understanding: For clarity level 1: \\[ E[\\hbox{price}] = \\beta_0 + \\beta_8\\hbox{carat} \\] \\(\\beta_0\\) is the intercept \\(\\beta_8\\) is the carat coefficient (the slope for carat) For clarity level 2: \\[ \\begin{align*} E[\\hbox{price}] &amp;= \\beta_0 + \\beta_1 \\times 1 + \\beta_8\\hbox{carat} \\\\ &amp;= (\\beta_0 + \\beta_1) + \\beta_8\\hbox{carat} \\end{align*} \\] \\(\\beta_0+\\beta_1\\) is the intercept \\(\\beta_8\\) is the carat coefficient (the slope for carat) For clarity level 8: \\[ \\begin{align*} E[\\hbox{price}] &amp;= \\beta_0 + \\beta_7 \\times 1 + \\beta_8\\hbox{carat} \\\\ &amp;= (\\beta_0 + \\beta_7) + \\beta_8\\hbox{carat} \\end{align*} \\] \\(\\beta_0+\\beta_7\\) is the intercept \\(\\beta_8\\) is the carat coefficient (the slope for carat) Observations about these clarity level-specific model formulas: The carat coefficient is always the same: \\(\\beta_8\\) The intercept changes for each level of clarity \\(\\beta_1\\) to \\(\\beta_7\\) indicate changes in the intercept, compared to the reference category: clarity level 1 This model (y ~ categorical+quantitative) is called a parallel lines model. Exercise 4: Add to your previous model by including carat as a predictor. mod2 &lt;- lm(price ~ clarity+carat, data = diamonds) summary(mod2) Write an expression for the average price of a clarity level 1 diamond that is zero carats. What coefficient does this allow you to interpret? Write an expression for the average price of a clarity level 1 diamond that is \\(C\\) carats. Write an expression for the average price of a clarity level 2 diamond that is \\(C\\) carats. Subtract your answer from part (b) from this expression. Based on this, give an interpretation of the clarityLevel_2 coefficient. Write an expression for the average price of a clarity level 3 diamond that is \\(C\\) carats. Subtract your answer from part (b) from this expression. Based on this, give an interpretation of the clarityLevel_3 coefficient. Pick any clarity level. Write an expression for the average price of two diamonds of that clarity level: (1) one with \\(C\\) carats and (2) one with \\(C+1\\) carats. Subtract (1) from (2), and thus interpret the carat coefficient. Compare the values of the clarityLevel coefficients in mod2 and mod1 and form a conclusion about the relationship between price and clarity. In general, a linear regression model with multiple predictors is called a multiple linear regression model. \\[ E[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p \\] Interpreting \\(\\beta_1\\): Case 1: \\(x_1 = a_1, \\qquad x_2 = a_2, \\ldots, x_p = a_p\\) \\[ E[y_1] = \\beta_0 + \\beta_1 a_1 + \\beta_2 a_2 + \\cdots + \\beta_p a_p \\] Case 2: \\(x_1 = a_1 + 1, x_2 = a_2, \\ldots, x_p = a_p\\) Value of \\(x_1\\) is 1 unit higher, but otherwise exactly the same as Case 1. \\[ E[y_2] = \\beta_0 + \\beta_1 (a_1+1) + \\beta_2 a_2 + \\cdots + \\beta_p a_p \\] We can compare the expected (average) response for case 1 and case 2: \\[ E[y_2] - E[y_1] = \\beta_1 \\] Thus, \\(\\beta_1\\) is the expected change in the response for every unit change in the predictor \\(x_1\\), holding constant the other predictors in the model. On average, if we increase \\(x_1\\) by 1, we expect the response to change by \\(\\beta_1\\), holding constant the other predictors in the model. General strategy for interpreting regression coefficients Interpreting a coefficient on a quantiative variable \\(x\\) Write an expression for the expected outcome for (1) a case that has \\(x = a\\) and (2) a case that has \\(x = a+1\\) Make all other predictors have the same value for the two cases Compare the expected outcomes for the two cases to interpret the coefficient Interpreting a coefficient for an indicator variable for a categorical variable \\(x\\) Write an expression for the expected outcome for (1) a case that has \\(x\\) equal to the reference category and (2) a case that has \\(x\\) equal to the category of interest (the one corresponding to the coefficient) Make all other predictors have the same value for the two cases Compare the expected outcomes for the two cases to interpret the coefficient Exercise 5: What are we assuming about the form of the relationship between price and carat by including it in the model mod2? Make an appropriate plot to check. (Make sure to add + geom_smooth() to your plot.) Exercise 6: We can model nonlinear trends with linear regression models by adding polynomial terms. The code below fits the model \\[ E[\\hbox{price}] = \\beta_0 + \\beta_1\\hbox{clarityLevel_2} + \\cdots + \\beta_7\\hbox{clarityLevel_8} + \\beta_8\\hbox{carat} + \\beta_9\\hbox{carat}^2 \\] mod3 &lt;- lm(price ~ clarity + poly(carat, degree = 2, raw = TRUE), data = diamonds) summary(mod3) The coefficients poly(carat, degree = 2, raw = TRUE)1 and poly(carat, degree = 2, raw = TRUE)2 represent \\(\\beta_8\\) and \\(\\beta_9\\) respectively and aren’t really interpretable anymore. But the clarityLevel coefficients still have the same interpretation as in mod2. Do you reach the same conclusions about the relationship between clarity and price here as in mod2? How does the residual standard error compare in mod2 and mod3 and what does this indicate about the quality of the two models? (We can’t compare the Multiple R-squared, but we can compare the Adjusted R-squared. Coming up later!) Exercise 7: Now let’s consider color (Level 1 is best, and Level 7 is worst.) What do you expect the relationship between color and price to be. Make a plot to check. Fit a multiple linear regression model of price with clarity, carat, and color (Level 1 is best, and Level 7 is worst.) Interpret the intercept, clarityLevel_2, carat, and colorLevel_2 coefficients. If the intercept is not meaningful, what could we do to make it meaningful? For carat to confound the relationship between color and price, what two properties must it have? Contextually, do you think that is plausible? Make plots to check if the data support that hypothesis. Do the same for carat confounding the relationship between clarity and price. Write a few sentences summarizing the nature of the confounding role of carat in the clarity vs. price and color vs. price relationships. Exercise 8: (Extra practice if you have time.) Fit a multiple linear regression model of price with clarity, carat, color, and cut. Interpret the intercept, clarityLevel_2, carat, colorLevel_2, and cut2_Good coefficients. Summarize your overall findings from this model, and appropriately compare its quality to the previous models. "],
["interaction-models.html", "Topic 8 Interaction Models Learning Goals Discussion and Exercises", " Topic 8 Interaction Models Learning Goals Understand what research questions can be answered with interaction models Interpret coefficients in interaction models Connect visualizations to the coefficient estimates in interaction models Graphically/geometrically describe the “pictures” implied by different types of interaction models Discussion and Exercises A template RMarkdown document that you can start from is available here. We will continue looking at the diamonds dataset. carat: the weight of the diamond in carats (1 carat = 200 milligrams) price: price in US dollars cut: quality of the cut of the diamond (Fair, Good, Very Good, Premium, Ideal) color: Level 1 (best) to Level 7 (worst) clarity: Level 1 (worst) to Level 8 (best) x, y, and z: length, width, and depth respectively (in mm) depth: total depth percentage = z / mean(x, y) table: width of top of diamond relative to widest point library(readr) library(ggplot2) library(dplyr) diamonds &lt;- read_csv(&quot;https://www.dropbox.com/s/9c8jqda4pwaq8i1/diamonds.csv?dl=1&quot;) Warm-up: Consider the model of price as a function of clarity and carat. \\[ E[\\hbox{price}] = \\beta_0 + \\beta_1\\hbox{clarityLevel_2} + \\beta_2\\hbox{clarityLevel_3} + \\cdots + \\beta_7\\hbox{clarityLevel_8} + \\beta_8\\hbox{carat} \\] Why was this called a parallel lines model? Draw a picture of the relationships that this model implies and label the picture with the coefficients of the model. Exercise 1: We looked at a scatterplot of price versus carat and boxplots of price versus clarity, but we haven’t looked at all three variables simultaneously. Let’s enrich the scatterplot to additionally show clarity. We’ve seen that we can have x, y, color, and fill aesthetics (inside the aes()). How might we adapt the code below to show different color trend lines corresponding to the 8 clarity levels? ggplot(diamonds, aes(x = carat, y = price)) + geom_point() + geom_smooth() If the relationship between the response \\(y\\) and predictor \\(x_1\\) differs depending on the value / categories of another predictor \\(x_2\\), we say that \\(x_1\\) and \\(x_2\\) interact. We capture this in our models with an interaction term: \\(x_1 \\times x_2\\). \\[ E[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 \\times x_2 \\] The coefficient \\(\\beta_3\\) in front of the interaction term is called the interaction coefficient. For now, let’s work with a subset of the data that just has two clarity levels. \\[ E[\\hbox{price}] = \\beta_0 + \\beta_1\\hbox{clarity2} + \\beta_2\\hbox{carat} + \\beta_3\\hbox{clarity2}\\times\\hbox{carat} \\] diamonds_sub &lt;- diamonds %&gt;% filter(clarity %in% c(&quot;Level_1&quot;, &quot;Level_2&quot;)) mod1 &lt;- lm(price ~ clarity*carat, data = diamonds_sub) summary(mod1) ## ## Call: ## lm(formula = price ~ clarity * carat, data = diamonds_sub) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6051.1 -645.7 -112.8 485.6 6304.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1480.56 103.76 -14.27 &lt;2e-16 *** ## clarityLevel_2 -1937.78 108.04 -17.94 &lt;2e-16 *** ## carat 4209.79 72.51 58.06 &lt;2e-16 *** ## clarityLevel_2:carat 3660.46 76.76 47.69 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1247 on 9931 degrees of freedom ## Multiple R-squared: 0.911, Adjusted R-squared: 0.9109 ## F-statistic: 3.387e+04 on 3 and 9931 DF, p-value: &lt; 2.2e-16 Note: When you see a colon between two variables, the colon means a multiplication sign. (clarityLevel_2:carat is the product of the clarityLevel_2 indicator variable and the carat variable.) Model formula \\[ \\begin{align*} E[\\hbox{price}] &amp;= \\beta_0 + \\beta_1\\hbox{clarity2} + \\beta_2\\hbox{carat} + \\beta_3\\hbox{clarity2}\\times\\hbox{carat} \\\\ &amp;= -1480.56 - 1937.78\\,\\hbox{clarity2} + 4209.79\\,\\hbox{carat} + 3660.46\\,\\hbox{clarity2}\\times\\hbox{carat} \\end{align*} \\] Model formula for clarity level 1 \\[ \\begin{align*} E[\\hbox{price}] &amp;= -1480.56 - 1937.78\\times 0 + 4209.79\\,\\hbox{carat} + 3660.46\\times 0 \\times\\hbox{carat} \\\\ &amp;= -1480.56 + 4209.79\\,\\hbox{carat} \\end{align*} \\] Model formula for clarity level 2 \\[ \\begin{align*} E[\\hbox{price}] &amp;= -1480.56 - 1937.78\\times 1 + 4209.79\\,\\hbox{carat} + 3660.46\\times 1 \\times\\hbox{carat} \\\\ &amp;= (-1480.56 - 1937.78) + (4209.79 + 3660.46)\\,\\hbox{carat} \\\\ &amp;= -3418.34 + 7870.25\\,\\hbox{carat} \\end{align*} \\] It is very helpful to draw a picture labeled with the coefficients. Coefficient interpretations: Intercept \\(\\beta_0\\): -1480.56 The average price for a zero carat, level 1 clarity diamond. (Could have made more meaningful by centering carat, but generally not interested in the intercept.) \\(\\beta_2\\) for carat: 4209.79 The slope for the clarity level 1 model. On average, diamond price increases by $4209.79 per carat increase in level 1 clarity diamonds. (This rate of change is different for clarity level 2 diamonds.) \\(\\beta_1\\) for clarityLevel_2: -1937.78 The change in y-intercept going from clarity level 1 to 2. On average, zero carat diamonds that are clarity level 2 are worth $1937.78 less than zero carat, clarity level 1 diamonds. \\(\\beta_3\\) for the interaction term (clarityLevel_2:carat): 3660.46 The change in slope for the clarity level 2 vs. the clarity level 1 model. The average change in price per carat increase is 3660.46 ($/carat) higher for clarity level 2 vs. clarity level 1 diamonds. The slope is steeper for level 2. A carat is worth more for a level 2 clarity diamond. Exercise 2: Fit a model with interaction between clarity and carat for the full diamonds dataset. Write out the model formulas for clarity level 1, 2, … diamonds until you get the hang of it. Draw a picture with the model lines for clarity level 1, 2, … 8 diamonds. Annotate this picture with the model coefficients. Interpret the clarityLevel_3 and clarityLevel_3:carat coefficients. Repeat for level 4 and onwards until you get the hang of it. For which clarity level is a carat worth the most? How much is a carat worth on average? Exercise 3: Now let’s consider diamond color (Level 1 is best and Level 7 is worst). What research question are you exploring when you fit a model with interaction between color and carat? Generalize this to a broader context. Make a plot that allows you to explore potential interaction between color and carat. What are you looking to see in the plot? Do you think an interaction model is appropriate? Fit the interaction model, and focus on the coefficients of substantive interest. Summarize your overall conclusions. "]
]
