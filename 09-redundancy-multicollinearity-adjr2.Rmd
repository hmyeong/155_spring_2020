```{r 09_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE)
```

# Redundancy, Multicollinearity, and Adjusted R-squared

## Learning Goals {-}

- Explain when variables are redundant or multicollinear
- Relate redundancy and multicollinearity to coefficient estimates and $R^2$
- Explain why adjusted $R^2$ is preferable to multiple $R^2$ when comparing models with different numbers of predictors

<br><br><br><br>

## Discussion {-}

**A reminder of why we care about all of this...**

<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">By the end of the flight, he had made several requests, initiated several times, &amp; his behaviors had reduced quite a bit. The father was astounded â€“ clearly no one had ever tried an AAC approach with him. I gave him the paper &amp; showed him how to use it, and he nearly cried. 6/</p>&mdash; Rachel R. Romeo (@RachelRRomeo) <a href="https://twitter.com/RachelRRomeo/status/1166817559611658240?ref_src=twsrc%5Etfw">August 28, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<br>

<center>
<span style="font-size: 20px">Models help us make <b>useful comparisons</b>.</span>
</center>

<br><br><br><br>

**Why are multiple regression models so useful?**

Adding predictors to models...

- Predictive viewpoint: Helps us better predict the response
- Descriptive viewpoint: Helps us better understand the isolated (causal) effect of a variable by holding constant confounders

BUT we can't throw in predictors indiscriminately.

<br><br><br><br>

```{r echo=FALSE, message=FALSE, eval=TRUE}
library(readr)
library(dplyr)
library(ggplot2)

bodyfat <- read_csv("https://www.dropbox.com/s/x8iuh71px1vn2an/bodyfat.csv?dl=1")
bodyfat$fat <- bodyfat$fatSiri
```

**Context:** Measuring body fat accurately is difficult. It would be nice to be able
We have a dataset of physical measurements on males (height, neck, thigh, etc.) 

Below we create an inches version of the `abdomen` variable:

```{r eval=TRUE}
bodyfat <- bodyfat %>%
    mutate(abdomenInches = abdomen/2.54)
```

Consider the following 3 models. What do you think the Multiple $R^2$ values will look like?

```{r eval=TRUE}
mod1 <- lm(fat ~ abdomen, data = bodyfat)
mod2 <- lm(fat ~ abdomenInches, data = bodyfat)
mod3 <- lm(fat ~ abdomen+abdomenInches, data = bodyfat)
```




<br><br><br><br>

```{r eval=TRUE}
summary(mod1)$r.squared
summary(mod2)$r.squared
summary(mod3)$r.squared
```

They're all the same! This is because `abdomen` and `abdomenInches` are **redundant**. They contain exactly the same information and predict each other in a perfect line:

```{r echo=FALSE, message=FALSE, eval=TRUE}
ggplot(bodyfat, aes(x = abdomen, y = abdomenInches)) +
    geom_point() +
    labs(x = "Abdomen circumference (cm)", y = "Abdomen circumference (in)")
```

Let's look at the summary output for `mod3`:

```{r eval=TRUE}
summary(mod3)
```

**Question:** The `NA`'s for the `abdomenInches` coefficient can be read as "cannot be computed". Why do you think that is?




<br><br><br><br>




The dataset also contains a `hip` variable giving hip circumference in centimeters. Consider the following 3 models:

- `mod1`: `fat ~ abdomen` (R-squared = 0.66)
- `mod4`: `fat ~ hip` (R-squared = 0.39)
- `mod5`: `fat ~ abdomen + hip`

What do you think the R-squared for `mod5` will be?

a. Close to 1
b. Close to 0.66
c. Close to 0.39
d. Midway from 0.66 to 1

```{r echo=FALSE, eval=TRUE}
mod4 <- lm(fat ~ hip, data = bodyfat)
mod5 <- lm(fat ~ abdomen + hip, data = bodyfat)
```

<br><br><br><br>

Abdomen and hip circumference are not perfectly linearly related but they are very similar. For this reason, we describe `abdomen` and `hip` as being **multicollinear**.

```{r echo=FALSE, message=FALSE, eval=TRUE}
ggplot(bodyfat, aes(x = abdomen, y = hip)) +
    geom_point() +
    labs(x = "Abdomen circumference (cm)", y = "Hip circumference (cm)")
```

```{r eval=TRUE}
summary(mod5)
```

**Question:** How can we interpret the `hip` coefficient in `mod5`? Does this makes sense?


<br><br><br><br>

**Takeaway messages: redundancy and multicollinearity**

- Adding a redundant predictor to a model...
    - Does nothing to the $R^2$
    - Results in a senseless coefficient (that cannot even be estimated)
- Adding a multicollinear predictor to a model...
    - Minimally increases the $R^2$
    - Creates some concern over the interpretability of the coefficients




<br><br><br><br>

**What's wrong with the multiple R-squared?**

Adding a multicollinear predictor minimally increases $R^2$ but what about a useless predictor?

```{r echo=FALSE, eval=TRUE}
set.seed(161)
```

For illustration purposes, we'll look at a random sample of 10 of the males from the dataset:

```{r eval=TRUE}
bodyfat_subs <- bodyfat %>% sample_n(10)
```

Let's see if we can get our $R^2$ up to 1!

```{r eval=TRUE}
great_model <- lm(fat ~ abdomen+hip+thigh+knee+ankle+biceps+forearm+wrist, data = bodyfat_subs)
summary(great_model)
```

Darn! We probably just needed the magic predictor! Let's make it...

```{r eval=TRUE}
bodyfat_subs <- bodyfat_subs %>%
    mutate(magic = c(1,2,3,4,5,6,7,8,9,10))
```

...and fit the model:

```{r eval=TRUE}
best_model <- lm(fat ~ abdomen+hip+thigh+knee+ankle+biceps+forearm+wrist+magic, data = bodyfat_subs)
summary(best_model)
```

<br><br>

The multiple R-squared always goes up when a non-redundant variable is added to a model, even if that predictor is useless!

For this reason, using the multiple R-squared to compare models with different numbers of predictors is not fair. It is better to use **adjusted R-squared**:

$$ \hbox{Adj } R^2 = 1 - (1-R^2)\frac{n-1}{n-p-1} = R^2 - \hbox{penalty} $$

$n$ is the sample size, and $p$ is the number of non-intercept coefficients.

**Key takeaway:**

- The magnitude of the penalty increases as the number of predictors increases.
- So the adjusted R-squared won't increase unless the predictor increases the multiple R-squared sufficiently to surpass this penalty.
- Adjusted R-squared allows us to fairly compare the predictive ability of models with different numbers of predictors.

<br><br><br><br>

We have to take care when using polynomial terms to model nonlinearity:

```{r eval=TRUE}
poly_mod1 <- lm(fat ~ abdomen, data = bodyfat_subs)
poly_mod2 <- lm(fat ~ poly(abdomen, degree=2, raw=TRUE), data = bodyfat_subs)
poly_mod3 <- lm(fat ~ poly(abdomen, degree=3, raw=TRUE), data = bodyfat_subs)
poly_mod4 <- lm(fat ~ poly(abdomen, degree=4, raw=TRUE), data = bodyfat_subs)
poly_mod5 <- lm(fat ~ poly(abdomen, degree=5, raw=TRUE), data = bodyfat_subs)
```

```{r echo=FALSE, message=FALSE, eval=TRUE, fig.width=15, fig.height=8}
library(gridExtra)
bodyfat_subs$fitted1 <- fitted.values(poly_mod1)
bodyfat_subs$fitted2 <- fitted.values(poly_mod2)
bodyfat_subs$fitted3 <- fitted.values(poly_mod3)
bodyfat_subs$fitted4 <- fitted.values(poly_mod4)
bodyfat_subs$fitted5 <- fitted.values(poly_mod5)
p1 <- ggplot(bodyfat_subs, aes(x = abdomen, y = fat)) + geom_point() + geom_line(aes(x = abdomen, y = fitted1), col = "red", lwd = 2)
p2 <- ggplot(bodyfat_subs, aes(x = abdomen, y = fat)) + geom_point() + geom_line(aes(x = abdomen, y = fitted2), col = "red", lwd = 2)
p3 <- ggplot(bodyfat_subs, aes(x = abdomen, y = fat)) + geom_point() + geom_line(aes(x = abdomen, y = fitted3), col = "red", lwd = 2)
p4 <- ggplot(bodyfat_subs, aes(x = abdomen, y = fat)) + geom_point() + geom_line(aes(x = abdomen, y = fitted4), col = "red", lwd = 2)
p5 <- ggplot(bodyfat_subs, aes(x = abdomen, y = fat)) + geom_point() + geom_line(aes(x = abdomen, y = fitted5), col = "red", lwd = 2)
grid.arrange(p1,p2,p3,p4,p5, nrow = 2, ncol = 3)
```

- Degree 1: Multiple $R^2$ = `r summary(poly_mod1)$r.squared`. Adjusted $R^2$ = `r summary(poly_mod1)$adj.r.squared`.
- Degree 2: Multiple $R^2$ = `r summary(poly_mod2)$r.squared`. Adjusted $R^2$ = `r summary(poly_mod2)$adj.r.squared`.
- Degree 3: Multiple $R^2$ = `r summary(poly_mod3)$r.squared`. Adjusted $R^2$ = `r summary(poly_mod3)$adj.r.squared`.
- Degree 4: Multiple $R^2$ = `r summary(poly_mod4)$r.squared`. Adjusted $R^2$ = `r summary(poly_mod4)$adj.r.squared`.
- Degree 5: Multiple $R^2$ = `r summary(poly_mod5)$r.squared`. Adjusted $R^2$ = `r summary(poly_mod5)$adj.r.squared`.

<br>

This phenomenon of fitting the data too closely is called **overfitting**. If you're interested in learning how to build models for purely predictive (not descriptive) purposes, take *Statistical Machine Learning*!
